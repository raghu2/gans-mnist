{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from numpy import expand_dims\n",
    "from numpy import zeros\n",
    "from numpy import ones\n",
    "from numpy import vstack\n",
    "from numpy.random import randn\n",
    "from numpy.random import randint\n",
    "from keras.datasets.mnist import load_data\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Reshape\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import Conv2DTranspose\n",
    "from keras.layers import LeakyReLU\n",
    "from keras.layers import Dropout\n",
    "from matplotlib import pyplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_discriminator(in_shape=(28,28,1)):\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(64, (3,3), strides=(2, 2), padding='same', input_shape=in_shape))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Dropout(0.4))\n",
    "    model.add(Conv2D(64, (3,3), strides=(2, 2), padding='same'))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Dropout(0.4))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    opt = Adam(lr=0.0002, beta_1=0.5)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_generator(latent_dim):\n",
    "    model = Sequential()\n",
    "    n_nodes = 128 * 7 * 7\n",
    "    model.add(Dense(n_nodes, input_dim=latent_dim))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Reshape((7, 7, 128)))\n",
    "    model.add(Conv2DTranspose(128, (4,4), strides=(2,2), padding='same'))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Conv2DTranspose(128, (4,4), strides=(2,2), padding='same'))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Conv2D(1, (7,7), activation='sigmoid', padding='same'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_gan(g_model, d_model):\n",
    "    # make weights in the discriminator not trainable\n",
    "    d_model.trainable = False\n",
    "    model = Sequential()\n",
    "    model.add(g_model)\n",
    "    model.add(d_model)\n",
    "    opt = Adam(lr=0.0002, beta_1=0.5)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=opt)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_real_samples():\n",
    "    (trainX, _), (_, _) = load_data()\n",
    "    X = expand_dims(trainX, axis=-1)\n",
    "    X = X.astype('float32')\n",
    "    X = X / 255.0\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_real_samples(dataset, n_samples):\n",
    "    ix = randint(0, dataset.shape[0], n_samples)\n",
    "    X = dataset[ix]\n",
    "    # generate 'real' class labels (1)\n",
    "    y = ones((n_samples, 1))\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_latent_points(latent_dim, n_samples):\n",
    "    x_input = randn(latent_dim * n_samples)\n",
    "    x_input = x_input.reshape(n_samples, latent_dim)\n",
    "    return x_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_fake_samples(g_model, latent_dim, n_samples):\n",
    "    x_input = generate_latent_points(latent_dim, n_samples)\n",
    "    X = g_model.predict(x_input)\n",
    "    y = zeros((n_samples, 1))\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_performance(epoch, g_model, d_model, dataset, latent_dim, n_samples=100):\n",
    "    X_real, y_real = generate_real_samples(dataset, n_samples)\n",
    "    _, acc_real = d_model.evaluate(X_real, y_real, verbose=0)\n",
    "    x_fake, y_fake = generate_fake_samples(g_model, latent_dim, n_samples)\n",
    "    _, acc_fake = d_model.evaluate(x_fake, y_fake, verbose=0)\n",
    "    print('>Accuracy real: %.0f%%, fake: %.0f%%' % (acc_real*100, acc_fake*100))\n",
    "    filename = 'generator_model_%03d.h5' % (epoch + 1)\n",
    "    g_model.save(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(g_model, d_model, gan_model, dataset, latent_dim, n_epochs=10, n_batch=256):\n",
    "    bat_per_epo = int(dataset.shape[0] / n_batch)\n",
    "    half_batch = int(n_batch / 2)\n",
    "    for i in range(n_epochs):\n",
    "        for j in range(bat_per_epo):\n",
    "            X_real, y_real = generate_real_samples(dataset, half_batch)\n",
    "            X_fake, y_fake = generate_fake_samples(g_model, latent_dim, half_batch)\n",
    "            X, y = vstack((X_real, X_fake)), vstack((y_real, y_fake))\n",
    "            d_loss, _ = d_model.train_on_batch(X, y)\n",
    "            X_gan = generate_latent_points(latent_dim, n_batch)\n",
    "            y_gan = ones((n_batch, 1))\n",
    "            g_loss = gan_model.train_on_batch(X_gan, y_gan)\n",
    "            print('>%d, %d/%d, d=%.3f, g=%.3f' % (i+1, j+1, bat_per_epo, d_loss, g_loss))\n",
    "        if (i+1) % 10 == 0:\n",
    "            summarize_performance(i, g_model, d_model, dataset, latent_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W1014 08:36:06.033453 140132469212928 deprecation.py:237] From /usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py:4139: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W1014 08:36:06.057747 140132469212928 deprecation.py:506] From /usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "W1014 08:36:06.204531 140132469212928 deprecation.py:237] From /usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py:3376: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "/usr/local/lib/python3.5/dist-packages/keras/engine/training.py:490: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">1, 1/234, d=0.689, g=0.741\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/keras/engine/training.py:490: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">1, 2/234, d=0.680, g=0.763\n",
      ">1, 3/234, d=0.667, g=0.785\n",
      ">1, 4/234, d=0.660, g=0.813\n",
      ">1, 5/234, d=0.657, g=0.837\n",
      ">1, 6/234, d=0.642, g=0.851\n",
      ">1, 7/234, d=0.638, g=0.860\n",
      ">1, 8/234, d=0.633, g=0.875\n",
      ">1, 9/234, d=0.629, g=0.870\n",
      ">1, 10/234, d=0.635, g=0.852\n",
      ">1, 11/234, d=0.640, g=0.828\n",
      ">1, 12/234, d=0.645, g=0.799\n",
      ">1, 13/234, d=0.649, g=0.770\n",
      ">1, 14/234, d=0.657, g=0.745\n",
      ">1, 15/234, d=0.657, g=0.727\n",
      ">1, 16/234, d=0.650, g=0.716\n",
      ">1, 17/234, d=0.649, g=0.710\n",
      ">1, 18/234, d=0.639, g=0.704\n",
      ">1, 19/234, d=0.632, g=0.702\n",
      ">1, 20/234, d=0.626, g=0.699\n",
      ">1, 21/234, d=0.620, g=0.699\n",
      ">1, 22/234, d=0.609, g=0.698\n",
      ">1, 23/234, d=0.602, g=0.698\n",
      ">1, 24/234, d=0.598, g=0.698\n",
      ">1, 25/234, d=0.585, g=0.699\n",
      ">1, 26/234, d=0.584, g=0.699\n",
      ">1, 27/234, d=0.561, g=0.700\n",
      ">1, 28/234, d=0.549, g=0.701\n",
      ">1, 29/234, d=0.551, g=0.701\n",
      ">1, 30/234, d=0.534, g=0.702\n",
      ">1, 31/234, d=0.517, g=0.703\n",
      ">1, 32/234, d=0.514, g=0.704\n",
      ">1, 33/234, d=0.500, g=0.704\n",
      ">1, 34/234, d=0.492, g=0.706\n",
      ">1, 35/234, d=0.481, g=0.707\n",
      ">1, 36/234, d=0.474, g=0.708\n",
      ">1, 37/234, d=0.466, g=0.709\n",
      ">1, 38/234, d=0.460, g=0.711\n",
      ">1, 39/234, d=0.448, g=0.713\n",
      ">1, 40/234, d=0.436, g=0.715\n",
      ">1, 41/234, d=0.434, g=0.717\n",
      ">1, 42/234, d=0.425, g=0.719\n",
      ">1, 43/234, d=0.419, g=0.721\n",
      ">1, 44/234, d=0.411, g=0.723\n",
      ">1, 45/234, d=0.403, g=0.727\n",
      ">1, 46/234, d=0.395, g=0.730\n",
      ">1, 47/234, d=0.387, g=0.733\n",
      ">1, 48/234, d=0.386, g=0.737\n",
      ">1, 49/234, d=0.383, g=0.741\n",
      ">1, 50/234, d=0.376, g=0.746\n",
      ">1, 51/234, d=0.375, g=0.751\n",
      ">1, 52/234, d=0.367, g=0.758\n",
      ">1, 53/234, d=0.359, g=0.763\n",
      ">1, 54/234, d=0.357, g=0.771\n",
      ">1, 55/234, d=0.348, g=0.780\n",
      ">1, 56/234, d=0.346, g=0.788\n",
      ">1, 57/234, d=0.339, g=0.799\n",
      ">1, 58/234, d=0.336, g=0.810\n",
      ">1, 59/234, d=0.331, g=0.823\n",
      ">1, 60/234, d=0.323, g=0.837\n",
      ">1, 61/234, d=0.316, g=0.852\n",
      ">1, 62/234, d=0.306, g=0.868\n",
      ">1, 63/234, d=0.301, g=0.886\n",
      ">1, 64/234, d=0.292, g=0.906\n",
      ">1, 65/234, d=0.291, g=0.927\n",
      ">1, 66/234, d=0.276, g=0.950\n",
      ">1, 67/234, d=0.277, g=0.976\n",
      ">1, 68/234, d=0.260, g=1.003\n",
      ">1, 69/234, d=0.252, g=1.033\n",
      ">1, 70/234, d=0.240, g=1.065\n",
      ">1, 71/234, d=0.237, g=1.099\n",
      ">1, 72/234, d=0.224, g=1.135\n",
      ">1, 73/234, d=0.216, g=1.178\n",
      ">1, 74/234, d=0.206, g=1.216\n",
      ">1, 75/234, d=0.198, g=1.261\n",
      ">1, 76/234, d=0.185, g=1.304\n",
      ">1, 77/234, d=0.182, g=1.349\n",
      ">1, 78/234, d=0.170, g=1.404\n",
      ">1, 79/234, d=0.161, g=1.452\n",
      ">1, 80/234, d=0.155, g=1.508\n",
      ">1, 81/234, d=0.144, g=1.554\n",
      ">1, 82/234, d=0.137, g=1.610\n",
      ">1, 83/234, d=0.132, g=1.671\n",
      ">1, 84/234, d=0.125, g=1.721\n",
      ">1, 85/234, d=0.120, g=1.782\n",
      ">1, 86/234, d=0.113, g=1.838\n",
      ">1, 87/234, d=0.111, g=1.891\n",
      ">1, 88/234, d=0.097, g=1.941\n",
      ">1, 89/234, d=0.101, g=2.002\n",
      ">1, 90/234, d=0.092, g=2.043\n",
      ">1, 91/234, d=0.086, g=2.083\n",
      ">1, 92/234, d=0.093, g=2.062\n",
      ">1, 93/234, d=0.160, g=1.571\n",
      ">1, 94/234, d=1.480, g=0.128\n",
      ">1, 95/234, d=2.970, g=0.018\n",
      ">1, 96/234, d=2.746, g=0.030\n",
      ">1, 97/234, d=2.087, g=0.105\n",
      ">1, 98/234, d=1.411, g=0.325\n",
      ">1, 99/234, d=0.867, g=0.799\n",
      ">1, 100/234, d=0.570, g=1.361\n",
      ">1, 101/234, d=0.416, g=1.887\n",
      ">1, 102/234, d=0.364, g=2.240\n",
      ">1, 103/234, d=0.326, g=2.383\n",
      ">1, 104/234, d=0.309, g=2.418\n",
      ">1, 105/234, d=0.310, g=2.412\n",
      ">1, 106/234, d=0.277, g=2.378\n",
      ">1, 107/234, d=0.253, g=2.332\n",
      ">1, 108/234, d=0.254, g=2.253\n",
      ">1, 109/234, d=0.269, g=2.070\n",
      ">1, 110/234, d=0.268, g=1.866\n",
      ">1, 111/234, d=0.283, g=1.592\n",
      ">1, 112/234, d=0.300, g=1.401\n",
      ">1, 113/234, d=0.313, g=1.270\n",
      ">1, 114/234, d=0.336, g=1.191\n",
      ">1, 115/234, d=0.343, g=1.127\n",
      ">1, 116/234, d=0.358, g=1.066\n",
      ">1, 117/234, d=0.387, g=0.996\n",
      ">1, 118/234, d=0.403, g=0.956\n",
      ">1, 119/234, d=0.460, g=0.908\n",
      ">1, 120/234, d=0.531, g=0.762\n",
      ">1, 121/234, d=0.580, g=0.696\n",
      ">1, 122/234, d=0.636, g=0.695\n",
      ">1, 123/234, d=0.651, g=0.638\n",
      ">1, 124/234, d=0.684, g=0.642\n",
      ">1, 125/234, d=0.715, g=0.669\n",
      ">1, 126/234, d=0.741, g=0.661\n",
      ">1, 127/234, d=0.770, g=0.642\n",
      ">1, 128/234, d=0.774, g=0.655\n",
      ">1, 129/234, d=0.812, g=0.641\n",
      ">1, 130/234, d=0.834, g=0.650\n",
      ">1, 131/234, d=0.855, g=0.665\n",
      ">1, 132/234, d=0.892, g=0.677\n",
      ">1, 133/234, d=0.900, g=0.681\n",
      ">1, 134/234, d=0.894, g=0.735\n",
      ">1, 135/234, d=0.905, g=0.724\n",
      ">1, 136/234, d=0.884, g=0.767\n",
      ">1, 137/234, d=0.890, g=0.746\n",
      ">1, 138/234, d=0.867, g=0.785\n",
      ">1, 139/234, d=0.900, g=0.812\n",
      ">1, 140/234, d=0.875, g=0.817\n",
      ">1, 141/234, d=0.879, g=0.818\n",
      ">1, 142/234, d=0.870, g=0.820\n",
      ">1, 143/234, d=0.832, g=0.840\n",
      ">1, 144/234, d=0.845, g=0.821\n",
      ">1, 145/234, d=0.835, g=0.821\n",
      ">1, 146/234, d=0.819, g=0.850\n",
      ">1, 147/234, d=0.778, g=0.850\n",
      ">1, 148/234, d=0.793, g=0.866\n",
      ">1, 149/234, d=0.790, g=0.870\n",
      ">1, 150/234, d=0.780, g=0.863\n",
      ">1, 151/234, d=0.773, g=0.876\n",
      ">1, 152/234, d=0.764, g=0.880\n",
      ">1, 153/234, d=0.752, g=0.895\n",
      ">1, 154/234, d=0.741, g=0.882\n",
      ">1, 155/234, d=0.729, g=0.900\n",
      ">1, 156/234, d=0.718, g=0.879\n",
      ">1, 157/234, d=0.720, g=0.861\n",
      ">1, 158/234, d=0.715, g=0.876\n",
      ">1, 159/234, d=0.710, g=0.881\n",
      ">1, 160/234, d=0.711, g=0.863\n",
      ">1, 161/234, d=0.684, g=0.873\n",
      ">1, 162/234, d=0.693, g=0.865\n",
      ">1, 163/234, d=0.689, g=0.848\n",
      ">1, 164/234, d=0.692, g=0.880\n",
      ">1, 165/234, d=0.692, g=0.873\n",
      ">1, 166/234, d=0.670, g=0.890\n",
      ">1, 167/234, d=0.653, g=0.872\n",
      ">1, 168/234, d=0.656, g=0.876\n",
      ">1, 169/234, d=0.671, g=0.865\n",
      ">1, 170/234, d=0.665, g=0.883\n",
      ">1, 171/234, d=0.680, g=0.891\n",
      ">1, 172/234, d=0.661, g=0.876\n",
      ">1, 173/234, d=0.675, g=0.908\n",
      ">1, 174/234, d=0.690, g=0.884\n",
      ">1, 175/234, d=0.663, g=0.862\n",
      ">1, 176/234, d=0.683, g=0.866\n",
      ">1, 177/234, d=0.679, g=0.840\n",
      ">1, 178/234, d=0.694, g=0.793\n",
      ">1, 179/234, d=0.694, g=0.772\n",
      ">1, 180/234, d=0.714, g=0.738\n",
      ">1, 181/234, d=0.696, g=0.722\n",
      ">1, 182/234, d=0.719, g=0.680\n",
      ">1, 183/234, d=0.723, g=0.675\n",
      ">1, 184/234, d=0.713, g=0.685\n",
      ">1, 185/234, d=0.710, g=0.691\n",
      ">1, 186/234, d=0.744, g=0.710\n",
      ">1, 187/234, d=0.736, g=0.739\n",
      ">1, 188/234, d=0.729, g=0.776\n",
      ">1, 189/234, d=0.710, g=0.790\n",
      ">1, 190/234, d=0.714, g=0.789\n",
      ">1, 191/234, d=0.718, g=0.806\n",
      ">1, 192/234, d=0.697, g=0.773\n",
      ">1, 193/234, d=0.708, g=0.756\n",
      ">1, 194/234, d=0.689, g=0.737\n",
      ">1, 195/234, d=0.682, g=0.729\n",
      ">1, 196/234, d=0.676, g=0.720\n",
      ">1, 197/234, d=0.665, g=0.699\n",
      ">1, 198/234, d=0.656, g=0.700\n",
      ">1, 199/234, d=0.644, g=0.690\n",
      ">1, 200/234, d=0.635, g=0.708\n",
      ">1, 201/234, d=0.641, g=0.701\n",
      ">1, 202/234, d=0.619, g=0.711\n",
      ">1, 203/234, d=0.632, g=0.736\n",
      ">1, 204/234, d=0.618, g=0.757\n",
      ">1, 205/234, d=0.615, g=0.778\n",
      ">1, 206/234, d=0.607, g=0.810\n",
      ">1, 207/234, d=0.611, g=0.822\n",
      ">1, 208/234, d=0.597, g=0.828\n",
      ">1, 209/234, d=0.597, g=0.804\n",
      ">1, 210/234, d=0.617, g=0.794\n",
      ">1, 211/234, d=0.621, g=0.764\n",
      ">1, 212/234, d=0.607, g=0.740\n",
      ">1, 213/234, d=0.603, g=0.719\n",
      ">1, 214/234, d=0.616, g=0.713\n",
      ">1, 215/234, d=0.598, g=0.716\n",
      ">1, 216/234, d=0.594, g=0.705\n",
      ">1, 217/234, d=0.599, g=0.690\n",
      ">1, 218/234, d=0.600, g=0.687\n",
      ">1, 219/234, d=0.604, g=0.687\n",
      ">1, 220/234, d=0.615, g=0.696\n",
      ">1, 221/234, d=0.618, g=0.728\n",
      ">1, 222/234, d=0.622, g=0.765\n",
      ">1, 223/234, d=0.638, g=0.777\n",
      ">1, 224/234, d=0.631, g=0.795\n",
      ">1, 225/234, d=0.652, g=0.782\n",
      ">1, 226/234, d=0.660, g=0.794\n",
      ">1, 227/234, d=0.693, g=0.817\n",
      ">1, 228/234, d=0.706, g=0.827\n",
      ">1, 229/234, d=0.702, g=0.825\n",
      ">1, 230/234, d=0.720, g=0.822\n",
      ">1, 231/234, d=0.750, g=0.770\n",
      ">1, 232/234, d=0.770, g=0.731\n",
      ">1, 233/234, d=0.820, g=0.663\n",
      ">1, 234/234, d=0.823, g=0.604\n",
      ">2, 1/234, d=0.865, g=0.594\n",
      ">2, 2/234, d=0.882, g=0.566\n",
      ">2, 3/234, d=0.883, g=0.562\n",
      ">2, 4/234, d=0.867, g=0.614\n",
      ">2, 5/234, d=0.833, g=0.653\n",
      ">2, 6/234, d=0.827, g=0.675\n",
      ">2, 7/234, d=0.779, g=0.713\n",
      ">2, 8/234, d=0.753, g=0.725\n",
      ">2, 9/234, d=0.736, g=0.738\n",
      ">2, 10/234, d=0.710, g=0.737\n",
      ">2, 11/234, d=0.691, g=0.755\n",
      ">2, 12/234, d=0.677, g=0.764\n",
      ">2, 13/234, d=0.655, g=0.753\n",
      ">2, 14/234, d=0.645, g=0.756\n",
      ">2, 15/234, d=0.623, g=0.757\n",
      ">2, 16/234, d=0.608, g=0.758\n",
      ">2, 17/234, d=0.595, g=0.778\n",
      ">2, 18/234, d=0.592, g=0.780\n",
      ">2, 19/234, d=0.575, g=0.800\n",
      ">2, 20/234, d=0.578, g=0.800\n",
      ">2, 21/234, d=0.554, g=0.820\n",
      ">2, 22/234, d=0.559, g=0.820\n",
      ">2, 23/234, d=0.557, g=0.800\n",
      ">2, 24/234, d=0.548, g=0.834\n",
      ">2, 25/234, d=0.555, g=0.798\n",
      ">2, 26/234, d=0.559, g=0.782\n",
      ">2, 27/234, d=0.573, g=0.756\n",
      ">2, 28/234, d=0.605, g=0.731\n",
      ">2, 29/234, d=0.643, g=0.706\n",
      ">2, 30/234, d=0.664, g=0.723\n",
      ">2, 31/234, d=0.698, g=0.797\n",
      ">2, 32/234, d=0.676, g=0.909\n",
      ">2, 33/234, d=0.678, g=1.036\n",
      ">2, 34/234, d=0.665, g=1.093\n",
      ">2, 35/234, d=0.685, g=1.084\n",
      ">2, 36/234, d=0.703, g=1.032\n",
      ">2, 37/234, d=0.706, g=0.917\n",
      ">2, 38/234, d=0.709, g=0.847\n",
      ">2, 39/234, d=0.709, g=0.774\n",
      ">2, 40/234, d=0.709, g=0.727\n",
      ">2, 41/234, d=0.713, g=0.701\n",
      ">2, 42/234, d=0.730, g=0.665\n",
      ">2, 43/234, d=0.739, g=0.643\n",
      ">2, 44/234, d=0.751, g=0.627\n",
      ">2, 45/234, d=0.740, g=0.630\n",
      ">2, 46/234, d=0.721, g=0.619\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">2, 47/234, d=0.761, g=0.613\n",
      ">2, 48/234, d=0.752, g=0.634\n",
      ">2, 49/234, d=0.749, g=0.645\n",
      ">2, 50/234, d=0.743, g=0.659\n",
      ">2, 51/234, d=0.743, g=0.667\n",
      ">2, 52/234, d=0.755, g=0.673\n",
      ">2, 53/234, d=0.743, g=0.704\n",
      ">2, 54/234, d=0.747, g=0.703\n",
      ">2, 55/234, d=0.747, g=0.713\n",
      ">2, 56/234, d=0.739, g=0.705\n",
      ">2, 57/234, d=0.746, g=0.731\n",
      ">2, 58/234, d=0.744, g=0.705\n",
      ">2, 59/234, d=0.747, g=0.699\n",
      ">2, 60/234, d=0.736, g=0.679\n",
      ">2, 61/234, d=0.727, g=0.681\n",
      ">2, 62/234, d=0.736, g=0.683\n",
      ">2, 63/234, d=0.716, g=0.678\n",
      ">2, 64/234, d=0.728, g=0.693\n",
      ">2, 65/234, d=0.711, g=0.694\n",
      ">2, 66/234, d=0.711, g=0.692\n",
      ">2, 67/234, d=0.719, g=0.694\n",
      ">2, 68/234, d=0.709, g=0.693\n",
      ">2, 69/234, d=0.694, g=0.710\n",
      ">2, 70/234, d=0.698, g=0.700\n",
      ">2, 71/234, d=0.701, g=0.709\n",
      ">2, 72/234, d=0.690, g=0.717\n",
      ">2, 73/234, d=0.670, g=0.721\n",
      ">2, 74/234, d=0.680, g=0.721\n",
      ">2, 75/234, d=0.681, g=0.741\n",
      ">2, 76/234, d=0.688, g=0.755\n",
      ">2, 77/234, d=0.676, g=0.756\n",
      ">2, 78/234, d=0.662, g=0.734\n",
      ">2, 79/234, d=0.672, g=0.742\n",
      ">2, 80/234, d=0.661, g=0.757\n",
      ">2, 81/234, d=0.668, g=0.744\n",
      ">2, 82/234, d=0.678, g=0.747\n",
      ">2, 83/234, d=0.664, g=0.741\n",
      ">2, 84/234, d=0.673, g=0.765\n",
      ">2, 85/234, d=0.667, g=0.765\n",
      ">2, 86/234, d=0.672, g=0.765\n",
      ">2, 87/234, d=0.655, g=0.747\n",
      ">2, 88/234, d=0.665, g=0.766\n",
      ">2, 89/234, d=0.667, g=0.755\n",
      ">2, 90/234, d=0.664, g=0.761\n",
      ">2, 91/234, d=0.657, g=0.740\n",
      ">2, 92/234, d=0.671, g=0.764\n",
      ">2, 93/234, d=0.658, g=0.754\n",
      ">2, 94/234, d=0.675, g=0.763\n",
      ">2, 95/234, d=0.673, g=0.765\n",
      ">2, 96/234, d=0.658, g=0.740\n",
      ">2, 97/234, d=0.674, g=0.746\n",
      ">2, 98/234, d=0.677, g=0.755\n",
      ">2, 99/234, d=0.671, g=0.765\n",
      ">2, 100/234, d=0.670, g=0.761\n",
      ">2, 101/234, d=0.691, g=0.751\n",
      ">2, 102/234, d=0.689, g=0.745\n",
      ">2, 103/234, d=0.695, g=0.747\n",
      ">2, 104/234, d=0.681, g=0.740\n",
      ">2, 105/234, d=0.693, g=0.739\n",
      ">2, 106/234, d=0.685, g=0.731\n",
      ">2, 107/234, d=0.684, g=0.729\n",
      ">2, 108/234, d=0.690, g=0.710\n",
      ">2, 109/234, d=0.696, g=0.717\n",
      ">2, 110/234, d=0.692, g=0.723\n",
      ">2, 111/234, d=0.694, g=0.702\n",
      ">2, 112/234, d=0.698, g=0.718\n",
      ">2, 113/234, d=0.692, g=0.713\n",
      ">2, 114/234, d=0.698, g=0.705\n",
      ">2, 115/234, d=0.718, g=0.711\n",
      ">2, 116/234, d=0.706, g=0.703\n",
      ">2, 117/234, d=0.694, g=0.701\n",
      ">2, 118/234, d=0.709, g=0.705\n",
      ">2, 119/234, d=0.706, g=0.718\n",
      ">2, 120/234, d=0.710, g=0.714\n",
      ">2, 121/234, d=0.719, g=0.709\n",
      ">2, 122/234, d=0.699, g=0.704\n",
      ">2, 123/234, d=0.721, g=0.696\n",
      ">2, 124/234, d=0.721, g=0.702\n",
      ">2, 125/234, d=0.718, g=0.697\n",
      ">2, 126/234, d=0.710, g=0.703\n",
      ">2, 127/234, d=0.712, g=0.701\n",
      ">2, 128/234, d=0.696, g=0.702\n",
      ">2, 129/234, d=0.713, g=0.690\n",
      ">2, 130/234, d=0.703, g=0.709\n",
      ">2, 131/234, d=0.701, g=0.689\n",
      ">2, 132/234, d=0.695, g=0.709\n",
      ">2, 133/234, d=0.696, g=0.713\n",
      ">2, 134/234, d=0.701, g=0.716\n",
      ">2, 135/234, d=0.722, g=0.716\n",
      ">2, 136/234, d=0.706, g=0.703\n",
      ">2, 137/234, d=0.697, g=0.707\n",
      ">2, 138/234, d=0.708, g=0.708\n",
      ">2, 139/234, d=0.723, g=0.727\n",
      ">2, 140/234, d=0.703, g=0.724\n",
      ">2, 141/234, d=0.686, g=0.715\n",
      ">2, 142/234, d=0.698, g=0.711\n",
      ">2, 143/234, d=0.680, g=0.707\n",
      ">2, 144/234, d=0.699, g=0.730\n",
      ">2, 145/234, d=0.695, g=0.725\n",
      ">2, 146/234, d=0.686, g=0.724\n",
      ">2, 147/234, d=0.693, g=0.717\n",
      ">2, 148/234, d=0.684, g=0.700\n",
      ">2, 149/234, d=0.688, g=0.708\n",
      ">2, 150/234, d=0.689, g=0.707\n",
      ">2, 151/234, d=0.685, g=0.717\n",
      ">2, 152/234, d=0.697, g=0.728\n",
      ">2, 153/234, d=0.696, g=0.732\n",
      ">2, 154/234, d=0.682, g=0.742\n",
      ">2, 155/234, d=0.677, g=0.733\n",
      ">2, 156/234, d=0.692, g=0.747\n",
      ">2, 157/234, d=0.687, g=0.740\n",
      ">2, 158/234, d=0.679, g=0.739\n",
      ">2, 159/234, d=0.683, g=0.728\n",
      ">2, 160/234, d=0.674, g=0.726\n",
      ">2, 161/234, d=0.687, g=0.732\n",
      ">2, 162/234, d=0.676, g=0.722\n",
      ">2, 163/234, d=0.678, g=0.729\n",
      ">2, 164/234, d=0.682, g=0.721\n",
      ">2, 165/234, d=0.664, g=0.735\n",
      ">2, 166/234, d=0.669, g=0.738\n",
      ">2, 167/234, d=0.683, g=0.723\n",
      ">2, 168/234, d=0.678, g=0.726\n",
      ">2, 169/234, d=0.685, g=0.741\n",
      ">2, 170/234, d=0.688, g=0.744\n",
      ">2, 171/234, d=0.685, g=0.739\n",
      ">2, 172/234, d=0.669, g=0.742\n",
      ">2, 173/234, d=0.659, g=0.734\n",
      ">2, 174/234, d=0.680, g=0.723\n",
      ">2, 175/234, d=0.677, g=0.716\n",
      ">2, 176/234, d=0.680, g=0.712\n",
      ">2, 177/234, d=0.677, g=0.689\n",
      ">2, 178/234, d=0.679, g=0.714\n",
      ">2, 179/234, d=0.681, g=0.718\n",
      ">2, 180/234, d=0.691, g=0.718\n",
      ">2, 181/234, d=0.682, g=0.724\n",
      ">2, 182/234, d=0.684, g=0.735\n",
      ">2, 183/234, d=0.694, g=0.737\n",
      ">2, 184/234, d=0.704, g=0.724\n",
      ">2, 185/234, d=0.685, g=0.734\n",
      ">2, 186/234, d=0.699, g=0.746\n",
      ">2, 187/234, d=0.687, g=0.728\n",
      ">2, 188/234, d=0.686, g=0.730\n",
      ">2, 189/234, d=0.693, g=0.729\n",
      ">2, 190/234, d=0.697, g=0.726\n",
      ">2, 191/234, d=0.698, g=0.723\n",
      ">2, 192/234, d=0.688, g=0.722\n",
      ">2, 193/234, d=0.701, g=0.700\n",
      ">2, 194/234, d=0.695, g=0.707\n",
      ">2, 195/234, d=0.693, g=0.706\n",
      ">2, 196/234, d=0.688, g=0.698\n",
      ">2, 197/234, d=0.698, g=0.700\n",
      ">2, 198/234, d=0.701, g=0.702\n",
      ">2, 199/234, d=0.692, g=0.698\n",
      ">2, 200/234, d=0.700, g=0.713\n",
      ">2, 201/234, d=0.706, g=0.720\n",
      ">2, 202/234, d=0.705, g=0.722\n",
      ">2, 203/234, d=0.701, g=0.722\n",
      ">2, 204/234, d=0.689, g=0.729\n",
      ">2, 205/234, d=0.698, g=0.722\n",
      ">2, 206/234, d=0.701, g=0.722\n",
      ">2, 207/234, d=0.694, g=0.714\n",
      ">2, 208/234, d=0.689, g=0.712\n",
      ">2, 209/234, d=0.700, g=0.710\n",
      ">2, 210/234, d=0.706, g=0.710\n",
      ">2, 211/234, d=0.696, g=0.707\n",
      ">2, 212/234, d=0.709, g=0.698\n",
      ">2, 213/234, d=0.700, g=0.699\n",
      ">2, 214/234, d=0.699, g=0.711\n",
      ">2, 215/234, d=0.697, g=0.715\n",
      ">2, 216/234, d=0.696, g=0.714\n",
      ">2, 217/234, d=0.686, g=0.724\n",
      ">2, 218/234, d=0.695, g=0.726\n",
      ">2, 219/234, d=0.706, g=0.731\n",
      ">2, 220/234, d=0.699, g=0.731\n",
      ">2, 221/234, d=0.694, g=0.734\n",
      ">2, 222/234, d=0.686, g=0.724\n",
      ">2, 223/234, d=0.698, g=0.718\n",
      ">2, 224/234, d=0.690, g=0.739\n",
      ">2, 225/234, d=0.692, g=0.714\n",
      ">2, 226/234, d=0.698, g=0.734\n",
      ">2, 227/234, d=0.682, g=0.732\n",
      ">2, 228/234, d=0.696, g=0.737\n",
      ">2, 229/234, d=0.691, g=0.731\n",
      ">2, 230/234, d=0.690, g=0.733\n",
      ">2, 231/234, d=0.686, g=0.730\n",
      ">2, 232/234, d=0.676, g=0.724\n",
      ">2, 233/234, d=0.692, g=0.722\n",
      ">2, 234/234, d=0.685, g=0.731\n",
      ">3, 1/234, d=0.690, g=0.724\n",
      ">3, 2/234, d=0.678, g=0.721\n",
      ">3, 3/234, d=0.685, g=0.732\n",
      ">3, 4/234, d=0.687, g=0.735\n",
      ">3, 5/234, d=0.676, g=0.738\n",
      ">3, 6/234, d=0.688, g=0.744\n",
      ">3, 7/234, d=0.688, g=0.737\n",
      ">3, 8/234, d=0.683, g=0.732\n",
      ">3, 9/234, d=0.683, g=0.714\n",
      ">3, 10/234, d=0.680, g=0.717\n",
      ">3, 11/234, d=0.665, g=0.717\n",
      ">3, 12/234, d=0.681, g=0.711\n",
      ">3, 13/234, d=0.685, g=0.725\n",
      ">3, 14/234, d=0.674, g=0.726\n",
      ">3, 15/234, d=0.686, g=0.724\n",
      ">3, 16/234, d=0.699, g=0.727\n",
      ">3, 17/234, d=0.686, g=0.733\n",
      ">3, 18/234, d=0.690, g=0.721\n",
      ">3, 19/234, d=0.686, g=0.726\n",
      ">3, 20/234, d=0.690, g=0.731\n",
      ">3, 21/234, d=0.682, g=0.724\n",
      ">3, 22/234, d=0.697, g=0.727\n",
      ">3, 23/234, d=0.687, g=0.721\n",
      ">3, 24/234, d=0.688, g=0.706\n",
      ">3, 25/234, d=0.691, g=0.712\n",
      ">3, 26/234, d=0.690, g=0.710\n",
      ">3, 27/234, d=0.690, g=0.707\n",
      ">3, 28/234, d=0.683, g=0.696\n",
      ">3, 29/234, d=0.700, g=0.712\n",
      ">3, 30/234, d=0.693, g=0.698\n",
      ">3, 31/234, d=0.703, g=0.691\n",
      ">3, 32/234, d=0.707, g=0.716\n",
      ">3, 33/234, d=0.707, g=0.697\n",
      ">3, 34/234, d=0.696, g=0.705\n",
      ">3, 35/234, d=0.703, g=0.703\n",
      ">3, 36/234, d=0.705, g=0.718\n",
      ">3, 37/234, d=0.696, g=0.710\n",
      ">3, 38/234, d=0.703, g=0.700\n",
      ">3, 39/234, d=0.705, g=0.704\n",
      ">3, 40/234, d=0.698, g=0.721\n",
      ">3, 41/234, d=0.705, g=0.728\n",
      ">3, 42/234, d=0.709, g=0.714\n",
      ">3, 43/234, d=0.693, g=0.714\n",
      ">3, 44/234, d=0.709, g=0.726\n",
      ">3, 45/234, d=0.702, g=0.725\n",
      ">3, 46/234, d=0.697, g=0.707\n",
      ">3, 47/234, d=0.704, g=0.711\n",
      ">3, 48/234, d=0.699, g=0.724\n",
      ">3, 49/234, d=0.693, g=0.716\n",
      ">3, 50/234, d=0.694, g=0.716\n",
      ">3, 51/234, d=0.697, g=0.720\n",
      ">3, 52/234, d=0.687, g=0.724\n",
      ">3, 53/234, d=0.688, g=0.728\n",
      ">3, 54/234, d=0.679, g=0.736\n",
      ">3, 55/234, d=0.698, g=0.734\n",
      ">3, 56/234, d=0.681, g=0.729\n",
      ">3, 57/234, d=0.681, g=0.726\n",
      ">3, 58/234, d=0.677, g=0.747\n",
      ">3, 59/234, d=0.674, g=0.740\n",
      ">3, 60/234, d=0.678, g=0.738\n",
      ">3, 61/234, d=0.687, g=0.744\n",
      ">3, 62/234, d=0.676, g=0.735\n",
      ">3, 63/234, d=0.674, g=0.745\n",
      ">3, 64/234, d=0.674, g=0.755\n",
      ">3, 65/234, d=0.681, g=0.746\n",
      ">3, 66/234, d=0.682, g=0.760\n",
      ">3, 67/234, d=0.683, g=0.768\n",
      ">3, 68/234, d=0.670, g=0.754\n",
      ">3, 69/234, d=0.674, g=0.767\n",
      ">3, 70/234, d=0.670, g=0.754\n",
      ">3, 71/234, d=0.666, g=0.748\n",
      ">3, 72/234, d=0.668, g=0.755\n",
      ">3, 73/234, d=0.672, g=0.763\n",
      ">3, 74/234, d=0.674, g=0.754\n",
      ">3, 75/234, d=0.678, g=0.769\n",
      ">3, 76/234, d=0.677, g=0.755\n",
      ">3, 77/234, d=0.667, g=0.749\n",
      ">3, 78/234, d=0.678, g=0.747\n",
      ">3, 79/234, d=0.679, g=0.716\n",
      ">3, 80/234, d=0.670, g=0.729\n",
      ">3, 81/234, d=0.673, g=0.731\n",
      ">3, 82/234, d=0.687, g=0.722\n",
      ">3, 83/234, d=0.675, g=0.726\n",
      ">3, 84/234, d=0.686, g=0.726\n",
      ">3, 85/234, d=0.681, g=0.718\n",
      ">3, 86/234, d=0.679, g=0.722\n",
      ">3, 87/234, d=0.688, g=0.728\n",
      ">3, 88/234, d=0.680, g=0.735\n",
      ">3, 89/234, d=0.680, g=0.736\n",
      ">3, 90/234, d=0.692, g=0.745\n",
      ">3, 91/234, d=0.689, g=0.751\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">3, 92/234, d=0.685, g=0.754\n",
      ">3, 93/234, d=0.694, g=0.738\n",
      ">3, 94/234, d=0.688, g=0.733\n",
      ">3, 95/234, d=0.688, g=0.724\n",
      ">3, 96/234, d=0.691, g=0.711\n",
      ">3, 97/234, d=0.689, g=0.695\n",
      ">3, 98/234, d=0.686, g=0.715\n",
      ">3, 99/234, d=0.684, g=0.709\n",
      ">3, 100/234, d=0.691, g=0.711\n",
      ">3, 101/234, d=0.684, g=0.716\n",
      ">3, 102/234, d=0.699, g=0.707\n",
      ">3, 103/234, d=0.700, g=0.705\n",
      ">3, 104/234, d=0.681, g=0.701\n",
      ">3, 105/234, d=0.693, g=0.690\n",
      ">3, 106/234, d=0.683, g=0.709\n",
      ">3, 107/234, d=0.689, g=0.714\n",
      ">3, 108/234, d=0.690, g=0.726\n",
      ">3, 109/234, d=0.689, g=0.726\n",
      ">3, 110/234, d=0.685, g=0.731\n",
      ">3, 111/234, d=0.691, g=0.743\n",
      ">3, 112/234, d=0.683, g=0.729\n",
      ">3, 113/234, d=0.691, g=0.734\n",
      ">3, 114/234, d=0.683, g=0.732\n",
      ">3, 115/234, d=0.695, g=0.725\n",
      ">3, 116/234, d=0.683, g=0.730\n",
      ">3, 117/234, d=0.684, g=0.721\n",
      ">3, 118/234, d=0.685, g=0.710\n",
      ">3, 119/234, d=0.696, g=0.694\n",
      ">3, 120/234, d=0.675, g=0.697\n",
      ">3, 121/234, d=0.685, g=0.693\n",
      ">3, 122/234, d=0.674, g=0.685\n",
      ">3, 123/234, d=0.685, g=0.693\n",
      ">3, 124/234, d=0.686, g=0.684\n",
      ">3, 125/234, d=0.688, g=0.694\n",
      ">3, 126/234, d=0.684, g=0.712\n",
      ">3, 127/234, d=0.683, g=0.706\n",
      ">3, 128/234, d=0.673, g=0.716\n",
      ">3, 129/234, d=0.686, g=0.723\n",
      ">3, 130/234, d=0.685, g=0.729\n",
      ">3, 131/234, d=0.691, g=0.726\n",
      ">3, 132/234, d=0.693, g=0.721\n",
      ">3, 133/234, d=0.696, g=0.701\n",
      ">3, 134/234, d=0.690, g=0.711\n",
      ">3, 135/234, d=0.698, g=0.690\n",
      ">3, 136/234, d=0.689, g=0.688\n",
      ">3, 137/234, d=0.672, g=0.693\n",
      ">3, 138/234, d=0.684, g=0.699\n",
      ">3, 139/234, d=0.685, g=0.693\n",
      ">3, 140/234, d=0.685, g=0.697\n",
      ">3, 141/234, d=0.686, g=0.697\n",
      ">3, 142/234, d=0.690, g=0.701\n",
      ">3, 143/234, d=0.675, g=0.707\n",
      ">3, 144/234, d=0.677, g=0.713\n",
      ">3, 145/234, d=0.676, g=0.717\n",
      ">3, 146/234, d=0.675, g=0.720\n",
      ">3, 147/234, d=0.677, g=0.719\n",
      ">3, 148/234, d=0.685, g=0.707\n",
      ">3, 149/234, d=0.679, g=0.726\n",
      ">3, 150/234, d=0.686, g=0.723\n",
      ">3, 151/234, d=0.668, g=0.722\n",
      ">3, 152/234, d=0.685, g=0.716\n",
      ">3, 153/234, d=0.684, g=0.713\n",
      ">3, 154/234, d=0.680, g=0.714\n",
      ">3, 155/234, d=0.688, g=0.697\n",
      ">3, 156/234, d=0.676, g=0.691\n",
      ">3, 157/234, d=0.668, g=0.699\n",
      ">3, 158/234, d=0.681, g=0.709\n",
      ">3, 159/234, d=0.683, g=0.709\n",
      ">3, 160/234, d=0.676, g=0.714\n",
      ">3, 161/234, d=0.679, g=0.718\n",
      ">3, 162/234, d=0.690, g=0.710\n",
      ">3, 163/234, d=0.682, g=0.707\n",
      ">3, 164/234, d=0.702, g=0.721\n",
      ">3, 165/234, d=0.679, g=0.728\n",
      ">3, 166/234, d=0.674, g=0.728\n",
      ">3, 167/234, d=0.675, g=0.725\n",
      ">3, 168/234, d=0.695, g=0.728\n",
      ">3, 169/234, d=0.679, g=0.730\n",
      ">3, 170/234, d=0.677, g=0.721\n",
      ">3, 171/234, d=0.686, g=0.714\n",
      ">3, 172/234, d=0.678, g=0.720\n",
      ">3, 173/234, d=0.685, g=0.713\n",
      ">3, 174/234, d=0.690, g=0.698\n",
      ">3, 175/234, d=0.682, g=0.698\n",
      ">3, 176/234, d=0.687, g=0.704\n",
      ">3, 177/234, d=0.690, g=0.713\n",
      ">3, 178/234, d=0.692, g=0.707\n",
      ">3, 179/234, d=0.694, g=0.709\n",
      ">3, 180/234, d=0.690, g=0.707\n",
      ">3, 181/234, d=0.679, g=0.723\n",
      ">3, 182/234, d=0.691, g=0.727\n",
      ">3, 183/234, d=0.685, g=0.721\n",
      ">3, 184/234, d=0.691, g=0.729\n",
      ">3, 185/234, d=0.674, g=0.714\n",
      ">3, 186/234, d=0.680, g=0.707\n",
      ">3, 187/234, d=0.688, g=0.709\n",
      ">3, 188/234, d=0.688, g=0.707\n",
      ">3, 189/234, d=0.701, g=0.702\n",
      ">3, 190/234, d=0.693, g=0.700\n",
      ">3, 191/234, d=0.692, g=0.715\n",
      ">3, 192/234, d=0.686, g=0.724\n",
      ">3, 193/234, d=0.688, g=0.724\n",
      ">3, 194/234, d=0.688, g=0.717\n",
      ">3, 195/234, d=0.693, g=0.720\n",
      ">3, 196/234, d=0.686, g=0.726\n",
      ">3, 197/234, d=0.684, g=0.709\n",
      ">3, 198/234, d=0.686, g=0.719\n",
      ">3, 199/234, d=0.683, g=0.709\n",
      ">3, 200/234, d=0.681, g=0.713\n",
      ">3, 201/234, d=0.687, g=0.720\n",
      ">3, 202/234, d=0.693, g=0.728\n",
      ">3, 203/234, d=0.678, g=0.708\n",
      ">3, 204/234, d=0.688, g=0.711\n",
      ">3, 205/234, d=0.679, g=0.717\n",
      ">3, 206/234, d=0.683, g=0.734\n",
      ">3, 207/234, d=0.688, g=0.717\n",
      ">3, 208/234, d=0.685, g=0.729\n",
      ">3, 209/234, d=0.686, g=0.719\n",
      ">3, 210/234, d=0.679, g=0.723\n",
      ">3, 211/234, d=0.694, g=0.728\n",
      ">3, 212/234, d=0.688, g=0.723\n",
      ">3, 213/234, d=0.692, g=0.724\n",
      ">3, 214/234, d=0.694, g=0.710\n",
      ">3, 215/234, d=0.676, g=0.714\n",
      ">3, 216/234, d=0.688, g=0.699\n",
      ">3, 217/234, d=0.693, g=0.720\n",
      ">3, 218/234, d=0.681, g=0.716\n",
      ">3, 219/234, d=0.698, g=0.724\n",
      ">3, 220/234, d=0.683, g=0.732\n",
      ">3, 221/234, d=0.690, g=0.731\n",
      ">3, 222/234, d=0.699, g=0.737\n",
      ">3, 223/234, d=0.683, g=0.729\n",
      ">3, 224/234, d=0.682, g=0.734\n",
      ">3, 225/234, d=0.688, g=0.746\n",
      ">3, 226/234, d=0.691, g=0.721\n",
      ">3, 227/234, d=0.682, g=0.730\n",
      ">3, 228/234, d=0.687, g=0.712\n",
      ">3, 229/234, d=0.694, g=0.704\n",
      ">3, 230/234, d=0.685, g=0.702\n",
      ">3, 231/234, d=0.696, g=0.711\n",
      ">3, 232/234, d=0.693, g=0.698\n",
      ">3, 233/234, d=0.692, g=0.706\n",
      ">3, 234/234, d=0.683, g=0.705\n",
      ">4, 1/234, d=0.683, g=0.711\n",
      ">4, 2/234, d=0.692, g=0.716\n",
      ">4, 3/234, d=0.684, g=0.711\n",
      ">4, 4/234, d=0.700, g=0.733\n",
      ">4, 5/234, d=0.692, g=0.734\n",
      ">4, 6/234, d=0.690, g=0.744\n",
      ">4, 7/234, d=0.689, g=0.732\n",
      ">4, 8/234, d=0.684, g=0.733\n",
      ">4, 9/234, d=0.684, g=0.720\n",
      ">4, 10/234, d=0.693, g=0.716\n",
      ">4, 11/234, d=0.687, g=0.726\n",
      ">4, 12/234, d=0.687, g=0.711\n",
      ">4, 13/234, d=0.717, g=0.700\n",
      ">4, 14/234, d=0.692, g=0.696\n",
      ">4, 15/234, d=0.689, g=0.704\n",
      ">4, 16/234, d=0.695, g=0.710\n",
      ">4, 17/234, d=0.692, g=0.727\n",
      ">4, 18/234, d=0.699, g=0.729\n",
      ">4, 19/234, d=0.698, g=0.719\n",
      ">4, 20/234, d=0.700, g=0.729\n",
      ">4, 21/234, d=0.702, g=0.717\n",
      ">4, 22/234, d=0.699, g=0.722\n",
      ">4, 23/234, d=0.691, g=0.727\n",
      ">4, 24/234, d=0.703, g=0.722\n",
      ">4, 25/234, d=0.705, g=0.722\n",
      ">4, 26/234, d=0.686, g=0.713\n",
      ">4, 27/234, d=0.699, g=0.701\n",
      ">4, 28/234, d=0.692, g=0.708\n",
      ">4, 29/234, d=0.688, g=0.711\n",
      ">4, 30/234, d=0.688, g=0.708\n",
      ">4, 31/234, d=0.686, g=0.703\n",
      ">4, 32/234, d=0.691, g=0.702\n",
      ">4, 33/234, d=0.680, g=0.722\n",
      ">4, 34/234, d=0.698, g=0.729\n",
      ">4, 35/234, d=0.694, g=0.747\n",
      ">4, 36/234, d=0.682, g=0.734\n",
      ">4, 37/234, d=0.689, g=0.728\n",
      ">4, 38/234, d=0.686, g=0.718\n",
      ">4, 39/234, d=0.685, g=0.715\n",
      ">4, 40/234, d=0.685, g=0.717\n",
      ">4, 41/234, d=0.695, g=0.702\n",
      ">4, 42/234, d=0.681, g=0.711\n",
      ">4, 43/234, d=0.679, g=0.699\n",
      ">4, 44/234, d=0.685, g=0.685\n",
      ">4, 45/234, d=0.679, g=0.701\n",
      ">4, 46/234, d=0.696, g=0.714\n",
      ">4, 47/234, d=0.681, g=0.721\n",
      ">4, 48/234, d=0.693, g=0.727\n",
      ">4, 49/234, d=0.683, g=0.746\n",
      ">4, 50/234, d=0.696, g=0.733\n",
      ">4, 51/234, d=0.698, g=0.716\n",
      ">4, 52/234, d=0.690, g=0.712\n",
      ">4, 53/234, d=0.678, g=0.712\n",
      ">4, 54/234, d=0.690, g=0.710\n",
      ">4, 55/234, d=0.682, g=0.699\n",
      ">4, 56/234, d=0.693, g=0.701\n",
      ">4, 57/234, d=0.689, g=0.704\n",
      ">4, 58/234, d=0.692, g=0.686\n",
      ">4, 59/234, d=0.683, g=0.700\n",
      ">4, 60/234, d=0.698, g=0.702\n",
      ">4, 61/234, d=0.693, g=0.702\n",
      ">4, 62/234, d=0.695, g=0.720\n",
      ">4, 63/234, d=0.696, g=0.720\n",
      ">4, 64/234, d=0.698, g=0.721\n",
      ">4, 65/234, d=0.691, g=0.704\n",
      ">4, 66/234, d=0.691, g=0.708\n",
      ">4, 67/234, d=0.690, g=0.696\n",
      ">4, 68/234, d=0.689, g=0.696\n",
      ">4, 69/234, d=0.693, g=0.693\n",
      ">4, 70/234, d=0.685, g=0.696\n",
      ">4, 71/234, d=0.696, g=0.712\n",
      ">4, 72/234, d=0.687, g=0.717\n",
      ">4, 73/234, d=0.700, g=0.717\n",
      ">4, 74/234, d=0.697, g=0.701\n",
      ">4, 75/234, d=0.695, g=0.710\n",
      ">4, 76/234, d=0.691, g=0.696\n",
      ">4, 77/234, d=0.708, g=0.712\n",
      ">4, 78/234, d=0.700, g=0.721\n",
      ">4, 79/234, d=0.702, g=0.700\n",
      ">4, 80/234, d=0.695, g=0.694\n",
      ">4, 81/234, d=0.691, g=0.696\n",
      ">4, 82/234, d=0.702, g=0.706\n",
      ">4, 83/234, d=0.705, g=0.705\n",
      ">4, 84/234, d=0.705, g=0.714\n",
      ">4, 85/234, d=0.693, g=0.719\n",
      ">4, 86/234, d=0.692, g=0.722\n",
      ">4, 87/234, d=0.684, g=0.717\n",
      ">4, 88/234, d=0.699, g=0.724\n",
      ">4, 89/234, d=0.688, g=0.712\n",
      ">4, 90/234, d=0.700, g=0.717\n",
      ">4, 91/234, d=0.694, g=0.718\n",
      ">4, 92/234, d=0.696, g=0.709\n",
      ">4, 93/234, d=0.683, g=0.722\n",
      ">4, 94/234, d=0.693, g=0.709\n",
      ">4, 95/234, d=0.701, g=0.700\n",
      ">4, 96/234, d=0.694, g=0.716\n",
      ">4, 97/234, d=0.700, g=0.695\n",
      ">4, 98/234, d=0.690, g=0.704\n",
      ">4, 99/234, d=0.711, g=0.715\n",
      ">4, 100/234, d=0.698, g=0.717\n",
      ">4, 101/234, d=0.705, g=0.733\n",
      ">4, 102/234, d=0.685, g=0.739\n",
      ">4, 103/234, d=0.698, g=0.732\n",
      ">4, 104/234, d=0.690, g=0.715\n",
      ">4, 105/234, d=0.689, g=0.717\n",
      ">4, 106/234, d=0.688, g=0.693\n",
      ">4, 107/234, d=0.690, g=0.695\n",
      ">4, 108/234, d=0.693, g=0.693\n",
      ">4, 109/234, d=0.678, g=0.698\n",
      ">4, 110/234, d=0.685, g=0.698\n",
      ">4, 111/234, d=0.694, g=0.708\n",
      ">4, 112/234, d=0.689, g=0.729\n",
      ">4, 113/234, d=0.694, g=0.736\n"
     ]
    }
   ],
   "source": [
    "latent_dim = 100\n",
    "d_model = define_discriminator()\n",
    "g_model = define_generator(latent_dim)\n",
    "gan_model = define_gan(g_model, d_model)\n",
    "dataset = load_real_samples()\n",
    "train(g_model, d_model, gan_model, dataset, latent_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
